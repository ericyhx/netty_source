1、服务端的socket在哪里初始化的？
2、在哪里accept连接

Netty服务端启动
    1、创建服务端Channel：调用jdk底层的api去创建jdk的一个channel，然后netty将其包装成为自己的channel，同时创建一些基本
    组件，绑定在此channel上面；步骤如下：
        bind()[用户代码入口]
            initAndRegister()[初始化并注册]
                newChannel()[创建服务端channel]
                反射创建服务端channel：
                    newSocket()[通过jdk来创建底层jdk channel]
                    NioServerSocketChannelConfig()[tcp参数配置类]
                    AbstractNioChannel()[NioSocketChannel的父类调用]
                        configureBlocking(false)[阻塞模式，设置创建出来的channel是一个非阻塞的模式]
                        AbstractChannel()[创建channel所对应的组件：id,unsafe,pipeline]
    2、初始化服务端channel：创建完成后，netty会基于此channel做一些初始化的工作，比如初始化一些基本属性以及添加一些逻辑
    处理器
                初始化服务端channel
                init()[初始化服务端channel]
                    set ChannelOptions,ChannelAttrs
                    set ChildOptions,ChildAttrs[为新连接创建的channel设置属性]
                    config handler[配置服务端的pipeline]
                    add ServerBootstrapAcceptor[添加连接处理器，这个特殊的处理器，给accept到的新连接分配一个Nio的线程]

           总结：保存用户自定义的那些属性，然后通过这些属性创建一个连接接入器，连接接入器每次accept到一个新的连接后，都会
           使用这些属性对新的连接做一些配置，
    3、注册selector：netty将其底层的channel注册到事件轮训器selector上面，并把netty的服务端channel作为一个attachment绑定到
    jdk底层的服务端channel，这样在后续如果有事件轮训出来的话就可以拿到这个attachment，也就是netty封装的一个服务端channel
                注册selector：
                AbstractChannel.register(channel)[入口]
                    this.eventloop=eventLoop[绑定线程，对应的nio线程和当前的channel做一个绑定]
                    register0()[实际注册]
                        doRegister()[调用jdk底层注册，就是把当前jdk的一个channel注册到一个selector上面去]
                        invokeHandlerAddedIfNeeded()[主要负责一些事件的回调]
                        fireChannelRegistered()[channel注册成功的事件传播到用户的代码里面]
    4、端口绑定：前面三步完成之后，就可以做端口绑定工作，最终也是调到jdk底层的api，实现本地端口的监听
    这四个过程完成后，netty的服务端就启动起来了。
                端口绑定
                AbstractUnsafe.bind()[入口]
                    doBind()[将端口实际绑定到本地]
                        javaChannel().bind()[jdk底层绑定]
                    pipeline.fireChannelActive()[传播事件]
                        HeadContext.readIfIsAutoRead()[将之前注册到selector上的事件重新绑定为一个op_accept事件，这样有
                        新连接进来，selector就会轮训到一个accept事件，最终就会将这个事件交给netty来出来]
NioEventLoop启动流程
1、默认情况下，Netty服务端起多少个线程？何时启动？
    不设参数的时候，默认启动2*cpu核数的线程，在调用execute方法的时候，会判断调用是否在本线程，如果在本线程说明线程已经启动，
    如果是在外部线程，调用execute方法首先会调用startThread(),会判断当前线程是否有启动，如果没有启动那就启动这个线程。
2、Netty是如何解决jdk空轮训的bug的？
    netty是通过一个技术的方式去判断如果当前阻塞了一个select操作，实际上并没有花这么长时间，那么有可能这一次就触发了空轮训的bug，
    默认情况下，如果这个线程达到512次，然后就重建一个selector，把之前的selector上的所有key重新移交到新的selector，通过这个方式巧妙的
    解决jdk空轮训的bug
3、Netty是如何保证异步串行无锁化？
    netty在所有外部线程去调用inEventLoop()或者channel的一些方法的时候，通过inEventLoop()来判断得出是外部线程，这个情况下
    会将所有外部线程封装成为一个task丢到MpscQueue里面，然后在NioEventLoop执行逻辑的第三过程这些task会被挨个执行
    NioEventLoop创建
        new NioEventLoopGroup()[创建线程组，默认是2*cpu]
            new ThreadPerTaskExecutor()[线程创建器]
                每次执行任务都会创建一个线程实体
                NioEventLoop线程命名规则nioEventLoop-1-xx
            for(){newChild()}[构造NioEventLoop]
                保存线程执行器ThreadPerTaskExecutor
                创建一个MpscQueue
                创建一个selector[轮训注册到NioEventLoop上的一个连接]
            chooserFactor.newChooser()[线程选择器，主要负责为新连接选择一个NioEventLoop]
                isPowerOfTwo()[判断是否是2的幂]
                    PowerOfTwoEventExecutorChooser[优化]
                        index++ & (length -1)[NioEventLoop的下标]
                    GenericEventExecutorChooser[普通]
                        abs(index++ % length)
    NioEventLoop启动
        服务端启动绑定端口
        bind()->execute(task)[入口，实际绑定的流程封装成为一个task，由服务端这个execute方法具体去执行]
            netty会判断调用execute方法的线程不是nio线程，于是会调用startThread方法开始尝试创建线程
            startThread()->doStartThread()[创建线程]
                ThreadPerTaskExecutor.execute()[每次执行任务的时候都会创建一个线程]
                    thread=Thread.currentThread()[保存的目的就是为了判断后续对NioEventLoop相关的执行线程是否是本身，
                    如果不是就封装成为一个task，扔到taskQueue里面去串行执行，保证线程安全]
                    NioEventLoop.run()[启动]
        新连接接入通过chooser绑定一个NioEventLoop
    NioEventLoop执行
        SingleThreadEventExecutor.this.run()
        NioEventLoop.run()
          run()->for(;;)
            select()[轮训注册到selector上面的连接io事件，检查是否有io事件]
                deadline以及任务穿插逻辑处理：首先计算本次执行select的一个截止时间，这个截止事件主要是根据NioEventLoop
                当前是否有定时任务需要处理，以及判断在select的时候是否有任务需要处理，也就是说在进行select的时候如果这个时候
                需要执行一个任务，select操作就会停止，否则就会进入下面的操作；
                阻塞式select：默认情况下是1s
                避免jdk空轮训的bug：
                总结：select会进行一个deadline的处理，然后判断当前有任务在里面就终止本次select，那么如果没有到截止事件以及当前
                taskQueue里面没有任务，就进行阻塞式的一个select操作，在阻塞式的select操作结束之后，会判断这次select操作是否真的
                阻塞了这么长时间，那如果没有阻塞这么长时间就表明可能触发了jdk的nio的空轮训的bug，接下来netty会判断触发空轮训的次数
                是否达到一个预值(512),如果达到了预值，就通过替换selector的操作避开了这个空轮训的bug
            processSelectKeys()[处理io事件]
                selected keySet的优化
                    selected操作每次都会把已经就绪状态的io事件添加到底层一个hashSet这样的一个数据结构，而netty会通过反射
                    的方式将HashSet替换为数组的一个实现，这样在任何情况下他操作的时间复杂度都是O(1),优于HashSet
                    processSelectKeysOptimized()
            runAllTasks()[处理异步任务队列，处理外部线程扔到taskQueue里面的任务]
                task的分类和添加：
                    普通task：MqscQueue
                    定时任务的task
                任务的聚合：首先会将定时任务里task聚合到普通任务
                任务的执行
            netty在执行这些任务的时候，首先会将定时任务聚合到普通任务里面，然后挨个去执行这些任务，并且在每次默认
            情况下执行64个任务之后计算一下当前的时间是否超过最大允许执行时间，，如果超过就直接中断，中断之后就进行
            下一次的NioEvenLoop的执行循环
总结：
NioEventLoop创建：
    用户代码在创建bossGroup和workerGroup的时候，NioEventLoop被创建，默认参数会创建2*cpu核数的NioEventLoop，每个NioEventLoop
都会有一个chooser进行线程逻辑的分配，这个chooser也会针对NioEventLoop的个数做一定优化，NioEventLoop在创建的时候会创建一个
selector和一个定时任务队列，在创建selector的时候，netty会通过反射的方式用数组实现来替换掉selected的两个hashSet数据结构；
NioEventLoop启动：
    NioEventLoop在首次调用execute方法的时候启动线程，这个线程是一个FastThreadLocalThread,启动线程之后，netty会将创建完成
的线程保存到成员变量，这样就能判断执行NioEventLoop里面的逻辑是否是本线程
NioEventLoop执行逻辑：
    NioEventLoop执行逻辑在run()里面，主要包括三个过程，第一个是检测io事件，第二个过程是处理这些io事件，最后执行任务队列；

Netty新连接接入
1、Netty是在哪里检测有新连接接入的？
    boss线程的第一个过程，轮训出accept事件，然后boss线程的第二个线程，通过jdk底层的channel.accept去创建这条连接
2、新连接是怎样注册到NioEventLoop线程的？
    boss线程调用chooser的next方法，拿到一个NioEventLoop，然后将这条连接注册到NioEventLoop的selector上面去
Netty新连接接入处理的逻辑
    检测新连接：新连接通过服务端channel绑定的selector轮训出accept事件；
        processSelectedKey(key,channel)[入口]
            NioMessageUnsafe.read()
                doReadMessages()[while循环调用，来创建新连接对象]
                    javaChannel().accept()[创建新连接最核心的方法,通过一个handler控制连接速率，默认最大一次读取16个连接]
    创建NioSocketChannel：检测新连接之后基于jdk的nio的channel创建一个Netty的NioSocketChannel，也就是客户端channel
        new NioSocketChannel(parent,ch)[入口，服务端channel通过反射的方式创建，为什么？]
            AbstractNioByteChannel(p,ch,op_read)[逐层调用父类的构造函数做一些事情]
                configureBlocking(false)&save op[首先配置此channel为非阻塞，然后将感兴趣的读事件保存到成员变量，方便后续逐层到seletor上]
                create id,unsafe,pipline[创建和此channel相关的一些组件，id作为唯一标识，unsafe作为底层数据的读写，pipeline作为业务逻辑的载体]
            new NioSocketChannelConfig(this, socket.socket())[创建一个和NioSocketChannel绑定的配置类]
                setTcpNoDelay(true)[禁用Nagle算法，小的数据包尽可能的会发出去，降低延时]
            Netty中Channel分类：
                NioServerSocketChannel->服务端channel
                NioSocketChannel->通过new关键字显示的创建客户端channel
                Unsafe->用于实现每种channel底层自己的协议
    分配线程及注册selector：Netty给客户端channel分配一个NioEventLoop，并且把这条channel注册到NioEventLoop对应的selector上，
    至此，这条channel后续的读写都由此NioEventLoop进行管理；
        ServerBootstrapAcceptor[连接接入器]
            添加chileHandler
            设置options和attrs
            选择NioEventLoop并注册selector
        服务端channel在检测到新连接并且创建完成客户端channel之后会调用一个连接器做一些处理，这些处理包括填充逻辑处理器，
        添加chileHandler，配置options和attrs，会调用chooser选择NioEventLoop进行绑定，绑定的时候并注册selector，这个时候
        不关心任何事件
    NioSocketChannel向selector注册读事件：注册的过程和服务端启动注册的accept事件，复用同一段逻辑；

Pipeline的相关逻辑
1、netty是如何判断ChannelHandler的类型的？
2、对于ChannelHandler的添加应该遵循什么样的顺序？
3、用户手动触发事件传播，不同的触发方式有什么样的区别？
    比如inBound事件： ctx.channel.write():head->tail
                      ctx.write():当前节点->tail

    pipeline的初始化
        pipeline在创建Channel的时候被创建；
        pipeline节点数据结构：ChannelHandlerContext
        pipeline的两大哨兵：head和tail
    添加删除ChannelHandler
        .addLast()
            判断是否重复添加（是否有Sharable注解或已经被添加过）
            创建节点并添加至链表(ChannelHandler里面的每一个节点都是一个ChannelHandlerContext)
            回调添加完成事件(通过一个回调事件告诉用户这个handler添加完成了，用户可以在回调方法里面做一些事件，
            最后把自身进行删除)
        remove(handler)
            找到节点
            链表的删除(可以通过标准的链表删除方式进行删除，不用关心这个节点是否是头或尾)
            回调删除Handler事件
    事件和异常的传播
        inBound事件的传播（与添加顺序正相关）
            何为inBound事件以及ChannelInboundHandler(inBound事件主要包括registered，active，read事件，ChannelInboundHandle
            在添加到一个pipeline的时候，会通过一个instance of关键词判断当前是一个inbound，所有的inbound事件都可以通过这个handler
            进行处理)
            ChannelRead事件的传播（通过添加ChannelHandler的顺序进行传播，如果通过pipeline去传播一个ChannelRead事件，他会
            通过head节点开始往下传播，而如果通过节点的ChannelHandlerContext调用我们的fireChannelRead事件传播，那他会从当
            前节点进行传播，最终会传播到tail节点，tail节点会进行释放）
            SimpleInBoundHandler处理器（最大的作用就是自动释放，不用用户关心释放问题）
        outBound事件的传播（与添加顺序逆相关）
            何为outBound事件以及ChannelOutboundHandler(通常指的是用户主动去发起的动作，不管是读、写还是关闭连接，这个事件
            都统称为outBound事件，ChannelOutboundHandler就是去处理这些outBound事件的handler)
            write()事件的传播：
                pipeline.channel.write():从tail节点开始向前传播
                pipeline的某个节点的write()方法，会从当前节点开始向前传播
        exception事件的传播
            异常的触发链:跟channelHandler的添加顺序是有关的，跟inboud或者outboud是没有关系的，在其中的一个handler读写数据
            发生异常，他会把你这个异常从当前节点开始，逐个往下传播，如果传播到最后一个节点，没有一个异常处理器的话，最终
            会落到tail节点，tail节点默认会给你一个告警，然后把这个异常信息进行一个打印
            异常处理的最佳实践：在pipeline的最后添加一个异常处理器，可以针对不同类型的异常进行分别处理
    总结：
    pipeline在服务端channel和客户端channel被创建的时候创建，创建pipeline的类是服务端和客户端channel的公共类AbstractChannel，
    pipeline的数据结构是双向链表结构，每一个节点都是一个ChannelHandlerContext，这个context包装了用户自定义的ChannelHandler，
    添加和删除ChannelHandler，最终都是在pipeline的链表结构中添加和删除对应的ChannelHandlerContext节点，而在添加ChannelHandlerContext
    的过程中，使用instance of关键词来判断ChannelHandler的类型，如果该节点实现了ChannelInBoundHandler，就会设置一个boolean
    类型的字段InBound为true来标识这个handler可以处理Inbound事件，同理outBound；默认情况下pipeline中会存在两种类型的节点，
    一个head，一个tail，head里面的unsafe负责实现channel的具体协议，而tail节点起到了终止事件和异常传播的作用；pipeline的传播
    机制分为三种：inBound、outBound、异常事件传播(从当前节点->tail)


ByteBuf
1、内存的类别有哪些？
2、如何减少多线程内存分配之间的竞争
    一个内存分配器里面维护着一个arean数组，所有的内存分配都在arean上进行，他通过一个pooledThreadCache对象将线程和arean进行
    一一绑定，默认情况下一个arean线程管理一个arean，这样就能左右多线程内存分配相互不受影响，说白了就是一个ThreadLocal的原理
3、不同大小的内存是如何进行分配的

    内存与内存管理器的抽象(ByteBuf)
        ByteBuf的结构
            0      <=      readerIndex   <=   writerIndex    <=    capacity (<=maxCapacity)
            read，write，set方法：read、write后指针都会移动
            mark和reset方法：目的是为了复原指针
        ByteBuf的分类
            Pooled和Unpooled
                 Pooled：从预先已经分配好的内存中去分配
                 Unpooled：直接去分配内存
            Unsafe和非Unsafe
                Unsafe：直接可以拿到bytebuf在jvm中的一个内存地址，调用jdk的unsafe进行读写；
                非Unsafe：不会依赖jdk底层的unsafe对象
            Heap和Direct
                Heap：直接在堆上进行内存分配，依赖底层的一个数组
                Direct：直接调用jdk的api进行内存分配，分配出来的内存是不受jvm控制的，需要手动释放内存
        ByteBufAllocator分析(netty的内存管理器)
            AbstractByteBufAllocator：
                 newDirectBuffer、 newHeapBuffer由子类去实现一个扩展功能
            ByteBufAllocator两大子类：
                PooledByteBufAllocator
                    拿到线程局部缓存PoolThreadCache
                    在线程局部缓存的Area上进行内存分配
                    directAreana分配direct内存的流程：
                        从对象池里面拿到PooledByteBuf进行复用
                        从缓存上进行内存分配
                        从内存堆里面进行内存分配
                UnpooledByteBufAllocator
                unsafe或非unsafe不需要用户去关系，netty底层会自己去判断有没有unsafe对象
    不同规格大小和不同类别的内存的分配策略
        内存规格介绍：
            0   <   tiny    <  512B    <=   small   <  8K  <=  normal    <= 16M <   huge
            内存分配是一chunk为单位进行申请，一个chunk为16M
            8K作为一个page进行分配
            0到8K作为一个subPage
        命中缓存的分配逻辑
            数据结构：MemoryRegionCache
                queue：      chunk handler       chunk handler       ......      chunk handler
                sizeClass:    tiny(0~512B)          small(512B~8K)                  normal(8K~16M)
                size:           N*16B            512B、1k、2K、4K                   8K、16K、32K...
            32K以上的内存是不进行缓存的
        命中缓存的分配流程：
            分段规格化
            找到对应size的MemoryRegionCache
            从queue中弹出一个entry给ByteBuf初始化
            将弹出的entry扔到对象池进行复用：通过Recycle对象池的机制进行管理，减少GC，减少一个对象的创建和销毁
            arena，chunk，page，subpage
            page级别的内存分配：allocateNormal()
                尝试在现有的chunk上分配
                创建一个chunk进行内存分配
                初始化PooledByteBuf
            subPage级别的内存分配：allocateTiny()
                定位一个Subpage对象
                初始化subpage：如果第一次创建，就需要进行初始化，划分成n等分
                初始化PooledByteBuf
    内存的回收过程
        连续的内存区段加到缓存
        标记连续的内存区段为为使用
        ByteBuf加到对象池
    总结：
        ByteBuf的api和分类
            ByteBuf最重要的就是几个read和write方法，AbstractByteBuf实现了ByteBuf的数据结构，抽象出一系列和数据读写相关的
            api给子类实现，ByteBuf的分类可以按照三个维度来分：一是堆内还是堆外，第二个是unsafe还是非unsafe，第三个是pooled
            和非pooled；堆内和堆外最容易区分，堆内基于byte字节数组内存进行分配，堆外基于jdk的directByteBuf进行内存分配，unsafe
            和非unsafe区别，就是unsafe是通过jdk的unsafe对象基于物理内存地址进行读写，而非unsafe直接调用jdk的api进行读写；
            pooled和unpooled的区别是unpooled每次分配内存都是直接申请内存，而pooled是预先分配好一整块内存，分配内存的时候直接用
            一定的算法从这块内存里面取出一定的连续内存；
        分配pooled内存的总步骤
            首先在线程池私有变量PooledThreadCache维护了一个缓冲空间，去找有没有之前使用过然后被释放的内存，如果有的话就直接
            基于这些连续内存进行分配，如果没有就用一定的算法在分配好的chunk进行内存分配
        不同规格的pooled内存分配和释放
            对于page级别的内存分配和释放是直接通过完全二叉树的标记来寻找某一段连续内存，而对于page级别一下的内存分配，首先是
            找到一个page，然后把此page按照子page大小进行划分，最后通过位图的方式来进行内存分配和释放，当然不管是page级别还是
            非page级别的内存，他这段内存被释放的时候，有可能会被加入到不同级别的缓存队列，供下一次分配使用




